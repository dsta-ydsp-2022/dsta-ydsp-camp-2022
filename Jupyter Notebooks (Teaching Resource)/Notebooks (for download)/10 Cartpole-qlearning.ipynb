{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time, math, random\n",
    "# using\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "# allows us to view the model as it is training, but it also slows down the process\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Bins for pole's angle = 6\n",
    "# Bins for pole's angular velocity = 12\n",
    "n_bins = ( 6 , 12 )\n",
    "\n",
    "# The actual pole angular velocity can go from -INF to +INF. We bin is to\n",
    "# between -50 rad/s and 50 rad/s\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "# We create a discretizer function that returns the bin\n",
    "# that the angle and pole angular velocity falls into\n",
    "est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "est.fit([lower_bounds, upper_bounds ])\n",
    "def discretizer(angle : float , pole_velocity : float ) -> tuple:\n",
    "    \"\"\"Convert continuous state into a bin (discrete state)\"\"\"\n",
    "    bins = est.transform([[angle, pole_velocity]])[0]\n",
    "    # return the bin for the angle, and the bin for the pole velocity\n",
    "    return (int(bins[0]), int(bins[1]))\n",
    "\n",
    "# Build a 3-dimensional table of 6 x 12 x 2 (because 2 possible actions)\n",
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "\n",
    "def policy( state : tuple ):\n",
    "    \"\"\"Choosing action based on policy\"\"\"\n",
    "    return np.argmax(Q_table[state])\n",
    "\n",
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    \"\"\"Temporal difference for updating Q-value of state-action pair\"\"\"\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value\n",
    "\n",
    "# Adaptive learning of Learning Rate\n",
    "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
    "    \"\"\"Decaying learning rate\"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "\n",
    "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
    "    \"\"\"Decaying exploration rate\"\"\"\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))\n",
    "\n",
    "n_episodes = 10000\n",
    "for e in range(n_episodes):\n",
    "    # Discretize state into buckets\n",
    "    observation, info = env.reset()\n",
    "    # Get current state as a tuple of 2 bins\n",
    "    # Send in only the angle and pole velocity\n",
    "    current_state = discretizer(observation[2], observation[3])\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    run_time = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        run_time += 1\n",
    "        # policy action\n",
    "        action = policy(current_state) # exploit\n",
    "\n",
    "        # insert random action in order to explore our state space more fully\n",
    "        if np.random.random() < exploration_rate(e) :\n",
    "            action = env.action_space.sample() # explore\n",
    "\n",
    "        # increment enviroment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_state = discretizer(obs[2], obs[3])\n",
    "\n",
    "        # Update Q-Table\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward , new_state )\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "\n",
    "        current_state = new_state\n",
    "\n",
    "        # Render the cartpole environment\n",
    "        env.render()\n",
    "    print(run_time)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
